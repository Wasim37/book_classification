{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意(attention)！开始做前必读项！\n",
    "所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件\n",
    "对于提供的数据集，不要改存储地方，也不要修改文件名和内容\n",
    "不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块\n",
    "写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！\n",
    "这次作业很重要，一定要完成！ 相信会有很多的收获！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关的包 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from gensim import models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500  # 表示样本表示最大的长度,表示降维之后的维度\n",
    "sentence_max_length = 3000  # 表示句子/样本在降维之前的维度\n",
    "Train_features3, Test_features3, Train_label3, Test_label3 = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取训练好的词嵌入向量和文本文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyedVectors.load_word2vec_format(file path, binary)  \n",
    "file path: 词向量模型路径  \n",
    "binary: text format or binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast_embedding输出词表的个数19592,w2v_embedding输出词表的个数19592\n",
      "取词向量成功\n"
     ]
    }
   ],
   "source": [
    "# ToDo\n",
    "# 通过models.KeyedVectors加载预训练好的embedding\n",
    "fast_embedding = models.KeyedVectors.load('model/fast_model_50000')\n",
    "w2v_embedding = models.KeyedVectors.load('model/w2v_model_50000')\n",
    "\n",
    "print(\"fast_embedding输出词表的个数{},w2v_embedding输出词表的个数{}\".format(\n",
    "    len(fast_embedding.wv.vocab.keys()), len(w2v_embedding.wv.vocab.keys())))\n",
    "\n",
    "print(\"取词向量成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据完成\n",
      "train.shape: (206316, 3)\n",
      "test.shape: (29474, 3)\n"
     ]
    }
   ],
   "source": [
    "# ToDo\n",
    "# 读取train_clean.tsv ，test_clean.tsv训练集和测试集文件\n",
    "# hint: 通过pandas中的read_csv读取数据集\n",
    "\n",
    "root_dir = '/home/user10000469/dataset/图书分类文本数据集/5813f3b1-617d-47ad-87e8-37a773a983af/file/归档/'\n",
    "train = pd.read_csv(root_dir + 'train_clean.csv', sep='\\t')\n",
    "test = pd.read_csv(root_dir + 'test_clean.csv', sep='\\t')\n",
    "print(\"读取数据完成\")\n",
    "print(\"train.shape:\", train.shape)\n",
    "print(\"test.shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将df中的label映射为数字标签并保存到labelIndex列中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelName = train.label.unique()  # 全部label列表\n",
    "labelIndex = list(range(len(labelName)))  # 全部label标签\n",
    "labelNameToIndex = dict(zip(labelName, labelIndex))  # label的名字对应标签的字典\n",
    "labelIndexToName = dict(zip(labelIndex, labelName))  # label的标签对应名字的字典\n",
    "train[\"labelIndex\"] = train.label.map(labelNameToIndex)\n",
    "test[\"labelIndex\"] = test.label.map(labelNameToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分数据完成\n"
     ]
    }
   ],
   "source": [
    "def query_cut(query):\n",
    "    '''\n",
    "    函数说明：该函数用于对输入的语句（query）按照空格进行切分\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 第一步：定义一个query_cut函数 将query按空格划分并返回，\n",
    "    # hint: 通过sqlit对query进行划分\n",
    "    query_list = query.split(' ')\n",
    "    return query_list\n",
    "\n",
    "# 第二步：然后train和test中的每一个样本都调用该函数，\n",
    "#      并将划分好的样本分别存储到train[\"queryCut\"]和test[\"queryCut\"]中\n",
    "train[\"queryCut\"] = train['text'].apply(query_cut)\n",
    "test[\"queryCut\"] = test['text'].apply(query_cut)\n",
    "print(\"切分数据完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除停用词\n"
     ]
    }
   ],
   "source": [
    "with open('data/stopwords.txt', \"r\") as f:\n",
    "    #ToDo\n",
    "    # 第一步：按行读取停用词文件\n",
    "    stopWords = f.readlines()\n",
    "\n",
    "def rm_stop_word(wordList):\n",
    "    '''\n",
    "    函数说明：该函数用于去除输入样本中的存在的停用词\n",
    "    Return: 返回去除停用词之后的样本\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 第二步：去除每个样本中的停用词并返回新的样本\n",
    "    return [word for word in wordList if word not in stopWords]\n",
    "    \n",
    "train[\"queryCutRMStopWord\"] = train[\"queryCut\"].apply(rm_stop_word)\n",
    "# dev[\"queryCutRMStopWord\"] = dev[\"text\"].apply(rm_stop_word)\n",
    "test[\"queryCutRMStopWord\"] = test[\"queryCut\"].apply(rm_stop_word)\n",
    "print(\"去除停用词\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category_id</th>\n",
       "      <th>labelIndex</th>\n",
       "      <th>queryCut</th>\n",
       "      <th>queryCutRMStopWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>杏林 芳菲 广东 中医药 曹磊 编著 的 杏林 芳菲 广东 中医药 注重 文化 内涵 的 挖...</td>\n",
       "      <td>文化</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[杏林, 芳菲, 广东, 中医药, 曹磊, 编著, 的, 杏林, 芳菲, 广东, 中医药, ...</td>\n",
       "      <td>[杏林, 芳菲, 广东, 中医药, 曹磊, 编著, 的, 杏林, 芳菲, 广东, 中医药, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>额吉 的 白云 优秀 蒙古文 文学作品 翻译 出版 工程 第五辑 散文 卷 散文集 额吉 的...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[额吉, 的, 白云, 优秀, 蒙古文, 文学作品, 翻译, 出版, 工程, 第五辑, 散文...</td>\n",
       "      <td>[额吉, 的, 白云, 优秀, 蒙古文, 文学作品, 翻译, 出版, 工程, 第五辑, 散文...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>叶赛宁 抒情诗 选 本书 收入 叶赛宁 的 206 首 抒情诗 , 多为 名篇 佳作 叶赛宁...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[叶赛宁, 抒情诗, 选, 本书, 收入, 叶赛宁, 的, 206, 首, 抒情诗, ,, ...</td>\n",
       "      <td>[叶赛宁, 抒情诗, 选, 本书, 收入, 叶赛宁, 的, 206, 首, 抒情诗, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>待 我 长发 及 腰 . 娶 我 可好 走过 唐诗 , 路过 宋词 , 相约 在 江南 的 ...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[待, 我, 长发, 及, 腰, ., 娶, 我, 可好, 走过, 唐诗, ,, 路过, 宋...</td>\n",
       "      <td>[待, 我, 长发, 及, 腰, ., 娶, 我, 可好, 走过, 唐诗, ,, 路过, 宋...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>公司债务 融资 及其 相关 金融 系统 本书 结合 经济 生活 热点 和 前沿 问题 , 对...</td>\n",
       "      <td>管理</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[公司债务, 融资, 及其, 相关, 金融, 系统, 本书, 结合, 经济, 生活, 热点,...</td>\n",
       "      <td>[公司债务, 融资, 及其, 相关, 金融, 系统, 本书, 结合, 经济, 生活, 热点,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  category_id  \\\n",
       "0  杏林 芳菲 广东 中医药 曹磊 编著 的 杏林 芳菲 广东 中医药 注重 文化 内涵 的 挖...    文化            6   \n",
       "1  额吉 的 白云 优秀 蒙古文 文学作品 翻译 出版 工程 第五辑 散文 卷 散文集 额吉 的...    文学            0   \n",
       "2  叶赛宁 抒情诗 选 本书 收入 叶赛宁 的 206 首 抒情诗 , 多为 名篇 佳作 叶赛宁...    文学            0   \n",
       "3  待 我 长发 及 腰 . 娶 我 可好 走过 唐诗 , 路过 宋词 , 相约 在 江南 的 ...    文学            0   \n",
       "4  公司债务 融资 及其 相关 金融 系统 本书 结合 经济 生活 热点 和 前沿 问题 , 对...    管理            1   \n",
       "\n",
       "   labelIndex                                           queryCut  \\\n",
       "0           0  [杏林, 芳菲, 广东, 中医药, 曹磊, 编著, 的, 杏林, 芳菲, 广东, 中医药, ...   \n",
       "1           1  [额吉, 的, 白云, 优秀, 蒙古文, 文学作品, 翻译, 出版, 工程, 第五辑, 散文...   \n",
       "2           1  [叶赛宁, 抒情诗, 选, 本书, 收入, 叶赛宁, 的, 206, 首, 抒情诗, ,, ...   \n",
       "3           1  [待, 我, 长发, 及, 腰, ., 娶, 我, 可好, 走过, 唐诗, ,, 路过, 宋...   \n",
       "4           2  [公司债务, 融资, 及其, 相关, 金融, 系统, 本书, 结合, 经济, 生活, 热点,...   \n",
       "\n",
       "                                  queryCutRMStopWord  \n",
       "0  [杏林, 芳菲, 广东, 中医药, 曹磊, 编著, 的, 杏林, 芳菲, 广东, 中医药, ...  \n",
       "1  [额吉, 的, 白云, 优秀, 蒙古文, 文学作品, 翻译, 出版, 工程, 第五辑, 散文...  \n",
       "2  [叶赛宁, 抒情诗, 选, 本书, 收入, 叶赛宁, 的, 206, 首, 抒情诗, ,, ...  \n",
       "3  [待, 我, 长发, 及, 腰, ., 娶, 我, 可好, 走过, 唐诗, ,, 路过, 宋...  \n",
       "4  [公司债务, 融资, 及其, 相关, 金融, 系统, 本书, 结合, 经济, 生活, 热点,...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.981 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_embedding: (33, 300)\n"
     ]
    }
   ],
   "source": [
    "labels = train['label'].drop_duplicates().values\n",
    "label_embedding = []\n",
    "for label in labels:\n",
    "    labelCut = jieba.lcut(label)\n",
    "    label_arr = [fast_embedding.wv.get_vector(cut)\n",
    "                         for cut in labelCut if cut in fast_embedding.wv.vocab.keys()]\n",
    "    label_arr = np.mean(label_arr, axis=0)\n",
    "    label_embedding.append(label_arr.tolist())\n",
    "\n",
    "label_embedding = np.array(label_embedding)\n",
    "print('label_embedding:', label_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_embedding_with_windows(embedding_matrix):\n",
    "    '''\n",
    "    函数说明：该函数用于获取在大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "    参数说明：\n",
    "        - embedding_matrix：样本中所有词构成的词向量矩阵\n",
    "    return: result_list 返回拼接而成的一维词向量\n",
    "    '''\n",
    "    result_list = []\n",
    "    # ToDo:\n",
    "    # 由于之前抽取的特征并没有考虑词与词之间交互对模型的影响，\n",
    "    # 对于分类模型来说，贡献最大的不一定是整个句子， 可能是句子中的一部分， 如短语、词组等等。 \n",
    "    # 在此基础上我们使用大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "\n",
    "    #https://blog.csdn.net/maverick_7/article/details/79102130\n",
    "    h_dim = embedding_matrix.shape[0]\n",
    "    n_gram = 5 if h_dim >=4 else h_dim + 1\n",
    "    for k in np.arange(2, n_gram):\n",
    "        gram_embedding = np.array([embedding_matrix[n:n+k].sum(axis=0) for n in np.arange(0, h_dim-k+1)])\n",
    "        max_embedding = np.max(gram_embedding, axis=0)\n",
    "        result_list = np.hstack((result_list, max_embedding))\n",
    "        mean_embedding = np.mean(gram_embedding, axis=0)\n",
    "        result_list = np.hstack((result_list, mean_embedding))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Label_embedding(word_matrix, label_embedding):\n",
    "    '''\n",
    "    函数说明：获取到所有类别的 label embedding， 与输入的 word embedding 矩阵相乘， 对其结果进行 softmax 运算，\n",
    "            对 attention score 与输入的 word embedding 相乘的结果求平均或者取最大\n",
    "            可以参考论文《Joint embedding of words and labels》获取标签空间的词嵌入\n",
    "            https://blog.csdn.net/lime_1002/article/details/88741510\n",
    "    parameters:\n",
    "    -- example_matrix(np.array 2D): denotes the matrix of words embedding\n",
    "    -- embedding(np.array 2D): denotes the embedding of all label in data\n",
    "    return: (np.array 1D) the embedding by join label and word\n",
    "    '''\n",
    "    result_embedding=[]\n",
    "    # To_Do \n",
    "    # 第一步：基于consin相似度计算word embedding向量与label embedding之间的相似度\n",
    "    similarity_matrix = np.dot(word_matrix, label_embedding.T)\n",
    "    # 第二步：通过softmax获取注意力分布\n",
    "    att_score =  np.max(np.array(similarity_matrix), axis=1) \n",
    "    # 第三步：将求得到的注意力分布与输入的word embedding相乘，并对结果进行最大化或求平均\n",
    "    att_embedding = np.multiply(word_matrix.T, att_score).T\n",
    "    result_embedding = np.hstack((np.max(np.array(att_embedding), axis=0), result_embedding))\n",
    "    result_embedding = np.hstack((np.mean(np.array(att_embedding), axis=0), result_embedding))\n",
    "    return result_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(query):\n",
    "    '''\n",
    "    函数说明：联合多种特征工程来构造新的样本表示，主要通过以下三种特征工程方法\n",
    "            第一：利用word-embedding的average pooling和max-pooling\n",
    "            第二：利用窗口size=2，3，4对word-embedding进行卷积操作，然后再进行max/avg-pooling操作\n",
    "            第二：利用类别标签的表示，增加了词语和标签之间的语义交互，以此达到对词级别语义信息更深层次的考虑\n",
    "            另外，对于词向量超过预定义的长度则进行截断，小于则进行填充\n",
    "    参数说明：\n",
    "    - query:数据集中的每一个样本\n",
    "    return: 返回样本经过哦特征工程之后得到的词向量\n",
    "    '''\n",
    "    global max_length\n",
    "    arr = []\n",
    "    # 加载fast_embedding,w2v_embedding\n",
    "    global fast_embedding, w2v_embedding, label_embedding\n",
    "    fast_arr = np.array([fast_embedding.wv.get_vector(s)\n",
    "                         for s in query if s in fast_embedding.wv.vocab.keys()])\n",
    "    \n",
    "    # 在fast_arr下滑动获取到的词向量\n",
    "    if len(fast_arr) > 0:\n",
    "        windows_fastarr = np.array(Find_embedding_with_windows(fast_arr))\n",
    "        result_attention_embedding = Find_Label_embedding(\n",
    "            fast_arr, label_embedding)\n",
    "    else:# 如果样本中的词都不在字典，则该词向量初始化为0\n",
    "        # 这里300表示训练词嵌入设置的维度为300\n",
    "        windows_fastarr = np.zeros(300) \n",
    "        result_attention_embedding = np.zeros(300)\n",
    "\n",
    "    fast_arr_max = np.max(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "    fast_arr_avg = np.mean(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "\n",
    "    fast_arr = np.hstack((fast_arr_avg, fast_arr_max))\n",
    "    # 将多个embedding进行横向拼接\n",
    "    arr = np.hstack((np.hstack((fast_arr, windows_fastarr)),\n",
    "                     result_attention_embedding))\n",
    "    global sentence_max_length\n",
    "    # 如果样本的维度大于指定的长度则需要进行截取或者拼凑,\n",
    "    result_arr = arr[:sentence_max_length] if len(arr) > sentence_max_length else np.hstack((\n",
    "        arr, np.zeros(int(sentence_max_length-len(arr)))))\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dimension_Reduction(Train, Test):\n",
    "    '''\n",
    "    函数说明：该函数通过PCA算法对样本进行降维，由于目前维度不是特别搞高 ，可以选择不降维。\n",
    "    参数说明：\n",
    "    - Train: 表示训练数据集\n",
    "    - Test: 表示测试数据集\n",
    "    Return: 返回降维之后的数据样本\n",
    "    '''\n",
    "    global max_length\n",
    "    #  To_Do \n",
    "    # 特征选择，由于经过特征工程得到的样本表示维度很高，因此需要进行降维 max_length表示降维之后的样本最大的维度。\n",
    "    # 这里通过PCA方法降维\n",
    "    pca = PCA(n_components=max_length) \n",
    "    pca.fit(Train)\n",
    "    pca_train = pca.fit_transform(Train) \n",
    "    pca.fit(Test)\n",
    "    pca_test = pca.fit_transform(Test) \n",
    "    return pca_train, pca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Embedding():\n",
    "    '''\n",
    "    函数说明：该函数用于获取经过特征工程之后的样本表示\n",
    "    Return:训练集特征数组(2D)，测试集特征数组(2D)，训练集标签数组（1D）,测试集标签数组（1D）\n",
    "    '''\n",
    "    print(\"获取样本表示中...\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    Train_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(train[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    Test_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(test[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "\n",
    "    print(\"获取样本词表示完成\")\n",
    "    # 通过PCA对样本表示进行降维\n",
    "    Train_features2, Test_features2 = Dimension_Reduction(\n",
    "        Train=Train_features2, Test=Test_features2)\n",
    "    Train_label2 = train[\"labelIndex\"]\n",
    "    Test_label2 = test[\"labelIndex\"]\n",
    "\n",
    "    print(\"加载训练好的词向量\")\n",
    "    print(\"Train_features.shape =\", Train_features2.shape)\n",
    "    print(\"Test_features.shape =\", Test_features2.shape)\n",
    "    print(\"Train_label.shape =\", Train_label2.shape)\n",
    "    print(\"Test_label.shape =\", Test_label2.shape)\n",
    "\n",
    "    return Train_features2, Test_features2, Train_label2, Test_label2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(Train_label, Test_label, Train_predict_label, Test_predict_label, model_name):\n",
    "    '''\n",
    "    函数说明：直接输出训练集和测试在模型上的准确率\n",
    "    参数说明：\n",
    "        - Train_label: 真实的训练集标签（1D）\n",
    "        - Test_labelb: 真实的测试集标签（1D）\n",
    "        - Train_predict_label: 模型在训练集上的预测的标签(1D)\n",
    "        - Test_predict_label: 模型在测试集上的预测标签（1D）\n",
    "        - model_name: 表示训练好的模型\n",
    "    Return: None\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 通过调用metrics.accuracy_score计算训练集和测试集上的准确率\n",
    "    train_accuracy = metrics.accuracy_score(Train_label, Train_predict_label)  \n",
    "    print(model_name+'_'+'Train accuracy %s' % train_accuracy )\n",
    "    # 输出测试集的准确率\n",
    "    test_accuracy = metrics.accuracy_score(Test_label, Test_predict_label)  \n",
    "    print(model_name+'_'+'test accuracy %s' % test_accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "第一步：完成LGBMClassifier模型中参数  \n",
    "第二步：根据joblib.dump保存训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_Train_model(Train_features, Test_features, Train_label, Test_label):\n",
    "    '''\n",
    "    函数说明：基于网格搜索优化的方法搜索模型最优参数，最后保存训练好的模型\n",
    "    参数说明：\n",
    "        - Train_features: 训练集特征数组（2D）\n",
    "        - Test_features: 测试集特征数组（2D）\n",
    "        - Train_label: 真实的训练集标签 (1D)\n",
    "        - Test_label: 真实的测试集标签（1D）\n",
    "    Return: None\n",
    "    '''\n",
    "#     parameters = {\n",
    "#         'max_depth': [5, 10, 15, 20, 25],\n",
    "#         'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "#         'n_estimators': [100, 500, 1000, 1500, 2000],\n",
    "#         'min_child_weight': [0, 2, 5, 10, 20],\n",
    "#         'max_delta_step': [0, 0.2, 0.6, 1, 2],\n",
    "#         'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n",
    "#         'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#         'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
    "#         'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],\n",
    "#         'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "#     }\n",
    "\n",
    "# 1.boosting_type=‘gbdt’# 提升树的类型 gbdt,dart,goss,rf\n",
    "# 2.num_leavel=32#树的最大叶子数，对比xgboost一般为2^(max_depth)\n",
    "# 3.max_depth=-1#最大树的深度\n",
    "# 4.learning_rate#学习率\n",
    "# 5.n_estimators=10: 拟合的树的棵树，相当于训练轮数\n",
    "# 6.subsample=1.0: 训练样本采样率 行\n",
    "# 7.colsample_bytree=1.0: 训练特征采样率 列\n",
    "# 8.subsample_freq=1: 子样本频率\n",
    "# 9.reg_alpha=0.0: L1正则化系数\n",
    "# 10.reg_lambda=0.0: L2正则化系数\n",
    "# 11.random_state=None: 随机种子数\n",
    "# 12.n_jobs=-1: 并行运行多线程核心数\n",
    "# 13.silent=True: 训练过程是否打印日志信息\n",
    "# 14.min_split_gain=0.0: 最小分割增益\n",
    "# 15.min_child_weight=0.001: 分支结点的最小权重\n",
    "\n",
    "\n",
    "# Init signature: lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100,\n",
    "#                                    subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, \n",
    "#                                    min_child_weight=0.001, min_child_samples=20, subsample=1.0,\n",
    "#                                    subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, \n",
    "#                                    random_state=None, n_jobs=-1, silent=True, importance_type='split', **kwargs)\n",
    "\n",
    "    parameters = {\n",
    "        'max_depth': [5, 10, 15, 20, 25],\n",
    "        'learning_rate': [0.01],\n",
    "        'n_estimators': [1500],\n",
    "        'min_child_weight': [2],\n",
    "        'max_delta_step': [0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n",
    "        'colsample_bytree': [0.5],\n",
    "        'reg_alpha': [0.25],\n",
    "        'reg_lambda': [0.2],\n",
    "        'scale_pos_weight': [0.2]\n",
    "    }\n",
    "    # 定义分类模型列表，这里仅使用LightGBM模型\n",
    "    models = [\n",
    "        lgb.LGBMClassifier(),\n",
    "    ]\n",
    "    # 遍历模型\n",
    "    for model in models:\n",
    "        model_name = model.__class__.  __name__\n",
    "        gsearch = GridSearchCV(\n",
    "            model, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "        gsearch.fit(Train_features, Train_label)\n",
    "        # 输出最好的参数\n",
    "        print(\"Best parameters set found on development set:{}\".format(\n",
    "            gsearch.best_params_))\n",
    "        Test_predict_label = gsearch.predict(Test_features)\n",
    "        Train_predict_label = gsearch.predict(Train_features)\n",
    "        Predict(Train_label, Test_label,\n",
    "                Train_predict_label, Test_predict_label, model_name)\n",
    "    # 保存训练好的模型\n",
    "    joblib.dump(gsearch.best_estimator_, 'best_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数,先求训练集和测试集的词向量，然后根据Grid搜索来找到最佳参数的分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取样本表示中...\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "Train_features, Test_features, Train_label, Test_label = Find_Embedding()\n",
    "Grid_Train_model(Train_features=Train_features, Test_features=Test_features,Train_label=Train_label, Test_label=Test_label)\n",
    "print(time.time()-t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
